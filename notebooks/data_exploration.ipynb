{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db27d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Starting Fraud Detection Analysis\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b0146f-0856-4c2a-b21e-57b2510baa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# STEP 1: DATA LOADING & INITIAL EXPLORATION\n",
    "# ================================\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    # Try to load from multiple possible locations\n",
    "    df = pd.read_csv('../data/fraud_data.csv')\n",
    "    print(\"‚úÖ Dataset loaded successfully!\")\n",
    "except:\n",
    "    try:\n",
    "        df = pd.read_csv('../data/PS_20174392719_1491204439457_log.csv')  # Original dataset name\n",
    "        print(\" Dataset loaded successfully!\")\n",
    "    except:\n",
    "        print(\" Dataset not found. Please ensure you have the fraud dataset in data/ folder\")\n",
    "        print(\"You can download it from: https://www.kaggle.com/datasets/jainilcoder/online-payment-fraud-detection\")\n",
    "        \n",
    "        # Create sample data for demonstration\n",
    "        print(\" Creating sample dataset for demonstration...\")\n",
    "        np.random.seed(42)\n",
    "        n_samples = 10000\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'step': np.random.randint(1, 744, n_samples),\n",
    "            'type': np.random.choice(['PAYMENT', 'TRANSFER', 'CASH_OUT', 'DEBIT', 'CASH_IN'], n_samples),\n",
    "            'amount': np.random.exponential(100000, n_samples),\n",
    "            'nameOrig': [f'C{i}' for i in range(n_samples)],\n",
    "            'oldbalanceOrg': np.random.exponential(50000, n_samples),\n",
    "            'newbalanceOrig': np.random.exponential(50000, n_samples),\n",
    "            'nameDest': [f'M{i}' for i in range(n_samples)],\n",
    "            'oldbalanceDest': np.random.exponential(50000, n_samples),\n",
    "            'newbalanceDest': np.random.exponential(50000, n_samples),\n",
    "            'isFraud': np.random.choice([0, 1], n_samples, p=[0.998, 0.002]),  # 0.2% fraud rate\n",
    "            'isFlaggedFraud': np.random.choice([0, 1], n_samples, p=[0.999, 0.001])\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6518d63c-c34a-4e14-8b46-98be8c05766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Dataset shape: {df.shape}\")\n",
    "print(\"\\n Dataset info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n First few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n Basic statistics:\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67d6540-6081-49eb-9767-9ddbed162ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# STEP 2: EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üîç EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n‚ùì Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Fraud distribution\n",
    "fraud_counts = df['isFraud'].value_counts()\n",
    "fraud_percentage = df['isFraud'].mean() * 100\n",
    "\n",
    "print(f\"\\nüö® Fraud Distribution:\")\n",
    "print(f\"Total transactions: {len(df):,}\")\n",
    "print(f\"Fraudulent: {fraud_counts[1]:,} ({fraud_percentage:.3f}%)\")\n",
    "print(f\"Legitimate: {fraud_counts[0]:,} ({100-fraud_percentage:.3f}%)\")\n",
    "\n",
    "# Visualization setup\n",
    "plt.style.use('seaborn-v0_8')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Fraud distribution\n",
    "axes[0,0].pie(fraud_counts.values, labels=['Legitimate', 'Fraud'], autopct='%1.1f%%', colors=['skyblue', 'red'])\n",
    "axes[0,0].set_title('Transaction Distribution')\n",
    "\n",
    "# 2. Fraud by transaction type\n",
    "fraud_by_type = df.groupby('type')['isFraud'].agg(['sum', 'count', 'mean']).reset_index()\n",
    "fraud_by_type['fraud_rate'] = fraud_by_type['mean'] * 100\n",
    "\n",
    "axes[0,1].bar(fraud_by_type['type'], fraud_by_type['fraud_rate'], color='orange')\n",
    "axes[0,1].set_title('Fraud Rate by Transaction Type')\n",
    "axes[0,1].set_ylabel('Fraud Rate (%)')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Amount distribution\n",
    "df[df['isFraud']==0]['amount'].hist(bins=50, alpha=0.7, label='Legitimate', ax=axes[1,0])\n",
    "df[df['isFraud']==1]['amount'].hist(bins=50, alpha=0.7, label='Fraud', ax=axes[1,0])\n",
    "axes[1,0].set_title('Amount Distribution')\n",
    "axes[1,0].set_xlabel('Amount')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].set_xlim(0, 500000)\n",
    "\n",
    "# 4. Correlation heatmap\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', ax=axes[1,1])\n",
    "axes[1,1].set_title('Feature Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Fraud by Transaction Type:\")\n",
    "print(fraud_by_type[['type', 'sum', 'fraud_rate']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aac94e-4362-4818-9296-10b9c85cc3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# STEP 3: DATA PREPROCESSING\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üîß DATA PREPROCESSING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Remove irrelevant features (high cardinality, low impact)\n",
    "columns_to_drop = ['nameOrig', 'nameDest']\n",
    "df_processed = df_processed.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"‚úÖ Removed columns: {columns_to_drop}\")\n",
    "\n",
    "# Feature Engineering\n",
    "print(\"\\nüî® Feature Engineering:\")\n",
    "\n",
    "# 1. Balance differences\n",
    "df_processed['balance_diff_orig'] = df_processed['oldbalanceOrg'] - df_processed['newbalanceOrig']\n",
    "df_processed['balance_diff_dest'] = df_processed['newbalanceDest'] - df_processed['oldbalanceDest']\n",
    "\n",
    "print(\"‚úÖ Created balance difference features\")\n",
    "\n",
    "# 2. One-hot encoding for transaction type\n",
    "type_dummies = pd.get_dummies(df_processed['type'], prefix='type')\n",
    "df_processed = pd.concat([df_processed, type_dummies], axis=1)\n",
    "df_processed = df_processed.drop('type', axis=1)\n",
    "\n",
    "print(\"‚úÖ Applied one-hot encoding for transaction types\")\n",
    "\n",
    "# 3. Handle target variable\n",
    "X = df_processed.drop(['isFraud'], axis=1)\n",
    "y = df_processed['isFraud']\n",
    "\n",
    "print(f\"\\nüìä Features shape: {X.shape}\")\n",
    "print(f\"üìä Target shape: {y.shape}\")\n",
    "print(f\"üìä Feature columns: {list(X.columns)}\")\n",
    "\n",
    "# 4. Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_features = ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', \n",
    "                     'oldbalanceDest', 'newbalanceDest', 'balance_diff_orig', 'balance_diff_dest']\n",
    "\n",
    "# Only scale features that exist in our dataset\n",
    "numerical_features = [col for col in numerical_features if col in X.columns]\n",
    "X[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "\n",
    "print(\"‚úÖ Scaled numerical features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a647894-8352-48a9-bdfe-8749449dc328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# STEP 4: HANDLE CLASS IMBALANCE\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚öñÔ∏è HANDLING CLASS IMBALANCE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Split data first\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"üìä Training set shape: {X_train.shape}\")\n",
    "print(f\"üìä Test set shape: {X_test.shape}\")\n",
    "\n",
    "# Apply SMOTE to training data only\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"\\nüîÑ After SMOTE:\")\n",
    "print(f\"üìä Balanced training set shape: {X_train_balanced.shape}\")\n",
    "print(f\"üìä Class distribution after SMOTE:\")\n",
    "print(pd.Series(y_train_balanced).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b74e59b-24b9-4458-99b8-dc31068cbd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# STEP 5: BUILD DEEP NEURAL NETWORK\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üß† BUILDING DEEP NEURAL NETWORK\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Model architecture based on the original research\n",
    "model = Sequential([\n",
    "    Dense(128, input_dim=X_train_balanced.shape[1], activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model architecture:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2474ae52-e130-4de7-adee-2da6c8ec8b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# STEP 6: TRAIN THE MODEL\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üèãÔ∏è TRAINING THE MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Calculate class weights for additional balance\n",
    "class_weights = {\n",
    "    0: 1.0,\n",
    "    1: len(y_train_balanced[y_train_balanced==0]) / len(y_train_balanced[y_train_balanced==1])\n",
    "}\n",
    "\n",
    "print(f\"üìä Class weights: {class_weights}\")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_balanced, y_train_balanced,\n",
    "    epochs=50,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657698ab-73a4-4987-a73c-a1b67f45c1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# STEP 7: MODEL EVALUATION\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä MODEL EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"üéØ Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Legitimate', 'Fraud']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\nüîç Confusion Matrix:\")\n",
    "print(f\"[[{cm[0,0]:6} {cm[0,1]:6}]\")\n",
    "print(f\" [{cm[1,0]:6} {cm[1,1]:6}]]\")\n",
    "\n",
    "print(f\"\\nExplanation:\")\n",
    "print(f\"True Negatives (TN): {cm[0,0]:,} - Legitimate correctly identified\")\n",
    "print(f\"False Positives (FP): {cm[0,1]:,} - Legitimate incorrectly flagged as fraud\")\n",
    "print(f\"False Negatives (FN): {cm[1,0]:,} - Fraud missed by model\")\n",
    "print(f\"True Positives (TP): {cm[1,1]:,} - Fraud correctly detected\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "#\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926305fa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud_env (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
